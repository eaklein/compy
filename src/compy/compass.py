# -*- coding: utf-8 -*-
"""
Created on Tue Oct 26 11:46:40 2021
@author: E. A. Klein
"""

import os
from bisect import bisect_left
from copy import deepcopy
import struct
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
from scipy.stats import gaussian_kde
from pprint import pprint

import click
import xmltodict

# set plotting environment
plt.close('all')
plt.rcParams.update({'font.size': 20})

# set LaTEX print parameters
plt.rcParams['text.usetex'] = True
plt.rcParams['xtick.direction'] = 'in'
plt.rcParams['ytick.direction'] = 'in'


class CompassRun:
    """
    A class for storing information from a CoMPASS data acquisition.

    Attributes
    ----------
    key : str
        unique run key assigned at creation
    folder : str
        DAQ folder location for CoMPASS project
    params : dictionary
        data acquisition parameters
    settings : dict
        CoMPASS settings read from xml file
    spectra : dict
        auto-generated spectra from CoMPASS
    data : dictionary
        un-/filtered data acquired and stored by CoMPASS

    Methods
    -------
    read_settings():
        Read CoMPASS project folder and extract settings for each run.
    read_spectra(modes=['E', 'TOF', 'PSD']):
        Read data from CoMPASS saved histogram.
    read_data(filtered=['unfiltered', 'filtered']):
        Read raw data from CoMPASS-generated files.
    add_tof(filtered=['unfiltered', 'filtered']):
        Add TOF column to raw data dataframe.
    user_filter(e_lo=95, e_hi=135, prompt=False):
        Perform user cut of energy spectrum.
    plot_tof(t_lo=0, t_hi=200, n_bins=400,
             filtered='unfiltered', norm=True, add=False):
        Plot manually calculated TOF spectrum.
    plot_spectrum():
        Plot spectrum auto-generated by CoMPASS.
    make_hist(filtered='unfiltered', mode='TOF', ch='CH0',
              val_lo=0, val_hi=200, n_bins=400):
        Create a histogram from TOF values.
    """

    def __init__(self, key, params=None, settings=None, spectra=None, data=None,
                 folder='C:/Users/Avram/Dropbox (MIT)/Resonances/data/CoMPASS/'):
        """Constructs all the necessary attributes for the compassRun class.

        Parameters
        ----------
            key : str
                unique run key assigned at creation
            folder : str
                DAQ folder location for CoMPASS project
            params : dictionary
                data acquisition parameters
            settings : dict
                CoMPASS settings read from xml file
            spectra : dict
                auto-generated spectra from CoMPASS
            data : dictionary
                un-/filtered data acquired and stored by CoMPASS
        """
        self.key = key
        self.folder = folder
        self.params = params
        self.settings = settings
        self.spectra = spectra
        self.data = data

    def read_settings(self):
        """Read CoMPASS project folder and extract settings for each run.

        """
        if self.settings is None:
            self.settings = {}
        try:
            xml_fname = self.folder + self.key + '/settings.xml'
            with open(xml_fname) as f:
                settings_xml = xmltodict.parse(f.read())
            # read board parameters
            for entry in (settings_xml
                          ['configuration']
                          ['board']
                          ['parameters']
                          ['entry']
                          ):
                self.settings[entry['key']] = entry['value']['value']['#text']
            # read acquisition time and convert to minutes
            self.settings['t_meas'] = float(
                settings_xml
                ['configuration']
                ['acquisitionMemento']
                ['timedRunDuration']
                )
            self.settings['t_meas'] /= 1000*60
            self.settings['file_format'] = (
                settings_xml
                ['configuration']
                ['acquisitionMemento']
                ['fileFormatList']
                )
        except FileNotFoundError:
            verboseprint(
                f'WARNING: Settings file could not be found for {self.key} '
                f'at {xml_fname}'
            )
            return
        # store certain settings in params dictionary
        self.params = {}
        # read measurement time
        self.params['t_meas'] = self.settings['t_meas']
        # read parameters for TOF spectra
        self.params['TOF'] = {}
        self.params['TOF']['n_bins'] = int(float(
            self.settings['SW_PARAMETER_DIFFERENCE_BINCOUNT']
        ))
        self.params['TOF']['t_lo'] = float(
            self.settings['SW_PARAMETER_TIME_DIFFERENCE_CH_T0']
        )
        self.params['TOF']['t_lo'] /= 1000  # convert to us
        self.params['TOF']['t_hi'] = float(
            self.settings['SW_PARAMETER_TIME_DIFFERENCE_CH_T1']
        )
        self.params['TOF']['t_hi'] /= 1000  # convert to us
        # read parameters for energy spectra
        self.params['E'] = {}
        self.params['E']['n_bins'] = int(
            self.settings['SRV_PARAM_CH_SPECTRUM_NBINS'].split('_')[1]
        )
        # read parameters for PSD spectra
        self.params['PSD'] = {}
        self.params['PSD']['n_bins'] = int(float(
            self.settings['SW_PARAMETER_PSDBINCOUNT']
        ))
        # read files from run directory
        file_fmt = '.' + self.settings['file_format'].lower()
        for filt_key in ['unfiltered', 'filtered']:
            filt_upper = filt_key.upper()
            self.params[filt_key] = {}
            folder_filt = self.folder + self.key + '/' + filt_upper + '/'
            try:
                os.listdir(folder_filt)
            except WindowsError:
                print(
                    f'WARNING: Cannot find {filt_upper} folder for {self.key}.'
                )
                break
            # read raw CH0 data location
            self.params[filt_key]['file_CH0'] = [
                file for file in os.listdir(folder_filt)
                if file.endswith(file_fmt) and ('CH0' in file)
            ]
            # read raw CH1 data location
            self.params[filt_key]['file_CH1'] = [
                file for file in os.listdir(folder_filt)
                if file.endswith(file_fmt) and ('CH1' in file)
            ]
            # read saved TOF spectra location
            self.params[filt_key]['file_data_TOF'] = [
                file for file in os.listdir(folder_filt)
                if file.endswith(".txt") and ('TOF' in file)
            ]
            # read saved E spectra location
            self.params[filt_key]['file_data_E'] = [
                file for file in os.listdir(folder_filt)
                if file.endswith(".txt") and ('CH0' in file) and ('E' in file)
            ]
            # read saved PSD spectra location
            self.params[filt_key]['file_data_PSD'] = [
                file for file in os.listdir(folder_filt)
                if file.endswith(".txt") and ('CH0' in file) and ('PSD' in file)
            ]

    def read_spectra(self, modes=['E', 'TOF', 'PSD']):
        """Read data from CoMPASS saved histograms.

        Parameters
        ----------
            modes: list[str]
                list of histogram types to read-in
        """
        if self.spectra is None:
            self.spectra = {}
        for filt_key in ['unfiltered', 'filtered']:
            self.spectra[filt_key] = {}
            for mode in modes:
                key_data = 'file_data_' + mode
                try:
                    self.spectra[filt_key][mode] = np.array(np.loadtxt(
                        self.folder + self.key + '/'
                        + filt_key.upper() + '/'
                        + max(self.params[filt_key][key_data])
                    ))
                    verboseprint('Read in CoMPASS spectrum for '
                                 f'{self.key} (mode: {mode}, {filt_key})')
                except:
                    print('WARNING: unable to open CoMPASS histogram for '
                          f'{self.key} (mode: {mode})')

    def read_data(self, filtered=['unfiltered', 'filtered']):
        """Read raw data from CoMPASS-generated files.

        Parameters
        ----------
            filtered: list[str]
                specifies un-/filtered folders from which to read data
        """
        if self.data is None:
            self.data = {}
        file_fmt = '.' + self.settings['file_format'].lower()
        for filt_key in filtered:
            self.data[filt_key] = {}
            # attempt to read Ch. 0 (detector) data if it exists
            if len(self.params[filt_key]['file_CH0']) > 0:
                verboseprint(
                    f'Reading {filt_key} CH0 data for key: {self.key}...',
                    end=""
                )
                try:
                    if file_fmt == '.csv':
                        self.data[filt_key]['CH0'] = pd.read_csv(
                            self.folder + self.key + '/'
                            + filt_key.upper() + '/'
                            + self.params[filt_key]['file_CH0'][0],
                            sep=';', on_bad_lines='skip'
                        )
                    elif file_fmt == 'bin':
                        file = open(self.folder + self.key + '/'
                                    + filt_key.upper() + '/'
                                    + self.params[filt_key]['file_CH0'][0],
                                    "rb")
                        byte = file.read()
                        data = struct.unpack(('<'+'HHQHHII'*(len(byte)//24)),
                                             byte)
                        self.data[filt_key]['CH0'] = pd.DataFrame(
                            np.reshape(np.array(data), [-1, 7])[:, 2:5],
                            columns=['TIMETAG', 'ENERGY', 'ENERGYSHORT']
                        )
                    if self.data[filt_key]['CH0'].empty:
                        print('file was empty.')
                    else:
                        print("Done!")
                except:
                    print(f'ERROR: unable to read in {filt_key} CH0 data '
                          f'for {self.key}.')
                    break
                # attempt to read Ch. 1 (pulse) data if TOF not yet calculated
                if 'TOF' in self.data[filt_key]['CH0']:
                    continue
                if len(self.params[filt_key]['file_CH1']) > 0:
                    verboseprint(
                        f'Reading {filt_key} CH1 data for key: {self.key}...',
                        end=""
                    )
                    if file_fmt == '.csv':
                        self.data[filt_key]['CH1'] = pd.read_csv(
                            self.folder + self.key + '/'
                            + filt_key.upper() + '/'
                            + self.params[filt_key]['file_CH0'][0],
                            sep=';', on_bad_lines='skip'
                        )
                    elif file_fmt == 'bin':
                        file = open(self.folder + self.key + '/'
                                    + filt_key.upper() + '/'
                                    + self.params[filt_key]['file_CH1'][0],
                                    "rb")
                        byte = file.read()
                        data = struct.unpack(('<'+'HHQHHII'*(len(byte)//24)),
                                             byte)
                        self.data[filt_key]['CH1'] = pd.DataFrame(
                            np.reshape(np.array(data), [-1, 7])[:, 2:5],
                            columns=['TIMETAG', 'ENERGY', 'ENERGYSHORT']
                        )
                    if self.data[filt_key]['CH1'].empty:
                        print('no data found.')
                    else:
                        print("Done!")
                else:
                    verboseprint(
                        f'Did not find {filt_key} CH1 data for key: {self.key}.'
                    )
            else:
                verboseprint(
                    f'Did not find {filt_key} CH0 data for key: {self.key}.'
                )

    def add_tof(self, filtered=['unfiltered', 'filtered']):
        """Add TOF column to raw data dataframe.

        Parameters
        ----------
            filtered: list[str]
                specifies un-/filtered folders from which to read data
        """
        for filt_key in filtered:
            # check if un-/filtered CH0 data present and TOF not yet calculated
            if ('CH0' in self.data[filt_key]) and (
                    'TOF' in self.data[filt_key]['CH0']):
                print('TOF already calculated!')
            elif 'CH0' in self.data[filt_key]:
                print('Could not calculate TOF. Ch. 0 data not found!')
            else:
                # attempt to read in Channel 1 (pulse) data if not yet done
                if ((len(self.params[filt_key]['file_CH1']) > 0) and
                        not(('CH1' not in self.data[filt_key])
                            or self.data[filt_key]['CH1'].empty)):
                    verboseprint(
                        f'Reading {filt_key} CH1 data for key: {self.key}.'
                    )
                    try:
                        self.data[filt_key]['CH1'] = pd.read_csv(
                            self.folder + self.key + '/'
                            + filt_key.upper() + '/'
                            + self.params[filt_key]['file_CH1'][0],
                            sep=';', on_bad_lines='skip'
                        )
                    except:
                        print(f'ERROR: unable to read in {filt_key} CH1 data '
                              f'for {self.key}.')
                        break
                else:
                    print(f'WARNING: TOF not calculated--{filt_key} CH1 data'
                          f'non-existent or empty for {self.key}.')
                    break
                # check that neither CH0/CH1 are empty and timing info saved
                if (all(not self.data[filt_key][ch].empty and
                        ('TIMETAG' in self.data[filt_key][ch]))
                        for ch in ['CH0', 'CH1']):
                    verboseprint(
                        f'Calculating {filt_key} TOF for {self.key}'
                    )
                    try:
                        # calculate TOF using TIMETAG info
                        self.data[filt_key]['CH0']['TOF'] = calc_TOF(
                            self.data[filt_key]['CH1']['TIMETAG'],
                            self.data[filt_key]['CH0']['TIMETAG']
                        )
                    except:
                        print('ERROR: issue arose calculating TOF.')
                        break
                    try:
                        # rewrite CH0 csv file with TOF added
                        self.data[filt_key]['CH0'].to_csv(
                            self.folder + self.key + '/'
                            + filt_key.upper() + '/'
                            + self.params[filt_key]['file_CH0'][0],
                            sep=';', index=False
                        )
                    except:
                        print('ERROR: issue arose writing TOF to file.')
                        break
                else:
                    print('ERROR: data empty or timetag not found.')


    def user_filter(self, e_lo=95, e_hi=135, prompt=False):
        """Perform user cut of energy spectrum.

        Parameters
        ----------
            e_lo: int
                energy low cut (ADC)
            e_hi : int
                energy hi cut (ADC)
            prompt: bool
                flag whether to ask user for cut bounds (True)
                or use defaults (False) if not provided
        """
        print(f'\nUser energy cut requested for TOF spectrum for {self.key}.')
        self.data['user'] = {}
        # choose to prompt user or else use default values
        if prompt:
            e_lo = click.prompt('Set e_lo channel for pulse area cut',
                                default=e_lo)
            e_hi = click.prompt('Set e_hi channel for pulse area cut',
                                default=e_hi)
        # copy all of CH1 pulse data
        self.data['user']['CH1'] = self.data['unfiltered']['CH1'].copy()
        # perform energy cut on unfiltered detector signal data
        self.data['user']['CH0'] = (
            self.data['unfiltered']['CH0']
            .loc[(self.data['unfiltered']['CH0']['ENERGY'] > e_lo) and
                 (self.data['unfiltered']['CH0']['ENERGY'] < e_hi)]
            .copy()
        )
        # if TOF not already calculated, calculate TOF
        if 'TOF' not in self.data['user']['CH0']:
            self.data['user']['CH0']['TOF'] = calc_TOF(
                self.data['unfiltered']['CH1']['TIMETAG'],
                self.data['user']['CH0']['TIMETAG']
            )
        print('Successfully performed user energy cut of TOF spectrum '
              f'for {self.key}.\n')

    def plot_tof(self, t_lo=0., t_hi=200., n_bins=400, color='blue',
                 filtered='unfiltered', norm=True, add=False):
        """Plot manually calculated TOF spectrum.

        Parameters
        ----------
            t_lo: float
                lower bound on TOF
            t_hi : float
                upper bound on TOF
            n_bins : int
                number of histogram bins
            color : str
                color of plot
            filtered : str
                choice of un-/filtered or user filter data to plot
            norm : bool
                whether to normalize data by measurement time
            add : bool
                whether to add plot to existing figure
        """
        if (('CH0' in self.data[filtered]) and
                (self.data[filtered]['CH0'].size >= 0)):
            try:
                x = self.data[filtered]['CH0'].TOF
            except:
                verboseprint(f'No TOF data to plot for {self.key}!')
                return
            if not add:
                plt.figure(figsize=(16, 9))
            if norm:
                weights = [1/self.params['t_meas']]*len(x)
                plt.ylabel('COUNTS/MINUTE')
            else:
                weights = [1]*len(x)
                plt.ylabel('COUNTS')
            plt.hist(x, range=[t_lo, t_hi], bins=n_bins, weights=weights,
                     histtype='step', lw=2, color=color,
                     label=(self.key + ' ' + filtered).replace('_', '-'))
            plt.xlim(t_lo, t_hi)
            plt.yscale('log')
            plt.legend(loc='upper right')
            plt.xlabel('TIME (us)')
            plt.tight_layout()
        else:
            verboseprint(f'No {filtered} CH0 data found for {self.key}!')

    def plot_spectrum(self, mode='TOF', filtered='unfiltered', add=False):
        """Plot spectrum auto-generated by CoMPASS.

        Parameters
        ----------
            mode: str
                list of histogram types to read-in
            filtered : str
                specifies un-/filtered folder from which to read data
            add : bool
                whether to add plot to existing figure
        """
        if add is False:
            plt.figure(figsize=(16, 9))
        y = self.spectra[filtered][mode]
        if mode == 'TOF':
            t_lo = self.params['TOF']['t_lo']
            t_hi = self.params['TOF']['t_hi']
            n_bins = self.params['TOF']['n_bins']
            x = np.linspace(t_lo, t_hi, n_bins)
            plt.xlim(self.params['TOF']['t_lo'], self.params['TOF']['t_hi'])
            plt.xlabel('TIME (us)')
        elif mode == 'PSD':
            x = np.arange(self.params['PSD']['n_bins'])
            plt.xlabel('PSD CHANNEL')
            plt.xlim(min(x), max(x))
        else:
            x = np.arange(1, len(y)+1)
            plt.xlim(0, len(y))
            plt.xlabel('CHANNEL')
        plt.errorbar(x, y, yerr=[np.sqrt(i) for i in y],
                     capsize=2, drawstyle='steps-mid',
                     label=self.key + ' ' + filtered)
        plt.ylabel('COUNTS')
        plt.ylim(bottom=1)
        plt.yscale('log')
        plt.legend(loc='upper right')
        plt.tight_layout()

    def make_hist(self, mode='TOF', filtered='unfiltered', ch='CH0',
                  val_lo=0., val_hi=200., n_bins=400):
        """Create a histogram from TOF values.

        Parameters
        ----------
            mode: str
                list of histogram types to read-in
            filtered : str
                specifies un-/filtered folder from which to read data
            ch : str
                specifies from which channel to read data
            val_lo : float
                lower bound on independent variable
            val_hi :
                upper bound on independent variable
            n_bins : int
                number of histogram bins

        Returns
        -------
            hist : array
                the values of the histogram
            bin_edges : array
                the bin edges
        """
        hist, bin_edges = np.histogram(self.data[filtered][ch][mode].to_numpy(),
                                       range=[val_lo, val_hi], bins=n_bins)
        return hist, bin_edges


def initialize(folders=None, keys=None):
    """Start up CoMPy, the CoMPASS Companion.

    Parameters
    ----------
        folders : list[str]
            DAQ folders from which to select keys
        keys : list[str]
            run keys chosen for processing data

    Returns
    -------
        folders : list[str]
            DAQ folders from which to select keys
        keys : list[str]
            run keys chosen for processing data
        VERBOSE : bool
            flag for printing verbose information about data processing
    """
    print('\nWelcome to CoMPy, the CoMPASS companion for Python!')
    if folders is None:
        folders = []
        while True:
            folder = input('Please enter a project folder path '
                           '(ending in DAQ):\n')
            if not folder:
                break
            else:
                try:
                    os.listdir(folder)
                    folders.append(folder)
                except FileNotFoundError:
                    print('The system cannot find the specified folder. '
                          'Please select another folder.')
    if keys is None:
        keys = select_keys(folders)
    VERBOSE = click.confirm('\nVerbose Mode?', default=True)
    return folders, keys, VERBOSE


def select_keys(folders):
    """Select keys to process.

    Parameters
    ----------
        folders: list[str]
            list of run folders to check for individual runs

    Returns
    -------
        keys_select : dict[list[str]]
            dictionary with project folders as keys,
            containing lists of individual run keys
    """
    keys_select = []
    n_folders = len(folders)
    for i, folder in enumerate(folders):
        # folder_key = folder.rsplit('/', maxsplit=3)[1]
        # keys_select[folder_key] = []
        keys_folder = [item for item in os.listdir(folder)
                if os.path.isdir(os.path.join(folder, item))]
        print('\nAvailable keys to process are: ')
        pprint(keys_folder, compact=True)
        bool_all = click.confirm('\nWould you like to process all keys?',
                                 default=True)
        # process all runs
        if bool_all:
            keys_new = [(key, folder) for key in keys_folder]
            keys_select.extend(keys_new)
            continue
        # manually select runs
        bool_date = click.confirm('\nWould you like to process by date?',
                                  default=True)
        if not bool_date:
            while True:
                n_keys = len(keys_select)
                key_input = input(
                    'Type \'options\' to see all available options.'
                    '\nPress \'enter\' to end key selection.'
                    '\nEnter key name: '
                )
                # click 'enter' to end key selection for folder
                if not key_input:
                    break
                # if 'enter', but on last folder and no keys selected
                if (not key_input) and (i == n_folders-1) and (n_keys == 0):
                    print('You must enter at least one key name!')
                # print all key options
                elif key_input == 'options':
                    print(keys_folder)
                # if key selected
                else:
                    # if user entry not available key, print warning
                    while key_input not in keys_folder:
                        key_input = input(
                            '\nThat key does not exist.'
                            '\nType \'options\' to see all available options.'
                            '\nPress \'enter\' to end key selection.'
                            '\nEnter key name: '
                        )
                        if key_input == 'options':
                            print(keys_folder)
                    # if good key, append to list
                    keys_select.append((key_input, folder))
        else:
            while True:
                n_keys = len(keys_select)
                date = input('(Note: \nPress \'enter\' to end key selection.)'
                             '\nEnter run date: ')
                # click 'enter' to end key selection
                if ((not date) and (n_keys == 0)):
                    break
                if (n_keys != 0 and
                        not any(key.startswith(date) for key in keys_folder)):
                    print('Bad key provided.')
                # if 'enter', but no keys selected
                elif ((not any(key.startswith(date) for key in keys_folder))
                      and n_keys == 0):
                    print('Bad key provided. '
                          'You must enter at least one key name!')
                # if good key, append to list
                elif any(key.startswith(date) for key in keys_folder):
                    for key in keys_folder:
                        if ((key.startswith(date)) and
                                ((key, folder) not in keys_select)):
                            keys_select.append((key, folder))
    return keys_select


def merge_copy(d1, d2):
    """ merge nested dicts """
    return {k: merge_copy(d1[k], d2[k])
            if k in d1 and isinstance(d1[k], dict) and isinstance(d2[k], dict)
            else merge_vals(d1[k], d2[k]) for k in d2}


def merge_vals(x, y):
    """ combine keys in dict """
    if x == y:
        return x
    if isinstance(x, list) and isinstance(y, list):
        return [*x, *y]
    return [x, y]


def merge_runs(keys, runs={}, merge_key=''):
    """ merge data from CoMPASS runs """
    # choose key for merged run
    if merge_key == '':
        merge_key = click.prompt('Which key should be used for the merged run?')
    # initialize merged run with first run provided
    run_merged = deepcopy(runs[keys[0]])
    run_merged.key = merge_key
    # iterate over additional keys to merge
    for key in keys[1:]:
        run = runs[key]
        # check settings
        if run.settings != run_merged.settings:
            print('Runs have different settings.')
            settings_key = keys[0]
            settings_key = click.prompt(
                "Please select:",
                type=click.Choice([run.key, run_merged.key],
                                  case_sensitive=False)
            )
            run_merged.settings = runs[settings_key].settings
        # check folder
        run_merged.folder = [[run.folder, run_merged.folder]
                             if run.folder != run_merged.folder else run.folder]
        # check if spectral parameters are equal
        if not all([run.params['E'] == run_merged.params['E'],
                    run.params['TOF'] == run_merged.params['TOF']]):
            print(
                'Spectra parameters are not the same for'
                f'{run.key} and {run_merged.key}.\n'
                f'Will keep spectra parameters from {run_merged.key}'
                ' but will not store any spectra.'
            )
        else:
            # if yes, merge spectra
            print(f'Merging spectra for {run.key} and {run_merged.key}.')
            run_merged.spectra = {}
            for filtered in ['unfiltered', 'filtered']:
                run_merged.spectra[filtered] = {}
                for key in (run.spectra[filtered].keys() and
                            run_merged.spectra[filtered].keys()):
                    run_merged.spectra[filtered][key] = [
                        xi + yi for xi, yi in zip(
                            run.spectra[filtered][key],
                            run_merged.spectra[filtered][key]
                        )
                    ]
        # merge params
        print(f'Merging parameters for {run.key} and {run_merged.key}.')
        t_meas = run_merged.params['t_meas']
        run_merged.params = merge_copy(run.params, run_merged.params)
        run_merged.params['t_meas'] = t_meas
        run_merged.params['t_meas'] += run.params['t_meas']
        # merge data
        for filtered in ['unfiltered', 'filtered']:
            for run in [run, run_merged]:
                if 'TOF' not in run.data[filtered]['CH0']:
                    run.add_tof(filtered=[filtered])
            run_merged.data[filtered]['CH0'] = pd.concat(
                [run.data[filtered]['CH0'],
                 run_merged.data[filtered]['CH0']],
                axis=0
            )
    runs[run_merged.key] = run_merged
    return run_merged


def process_runs(key_tuples, runs={}, verbose=0):
    """Read in settings, spectra, and data for specified runs."""
    bool_TOF = click.confirm('Would you like to manually calculate TOF?',
                             default='Y')
    runs = runs
    for (key, folder) in key_tuples:
        if key != 'CoMPASS':
            print('\n' + f'Processing Key: {key}...')
            run = CompassRun(key=key, folder=folder)
            run.read_settings()
            run.read_spectra()
            run.read_data()
            if bool_TOF:
                run.add_tof()
            runs[key] = run
    return runs


def calc_TOF(t_pulse, t_signal):
    "calculate TOF from pulse and signal time arrays"
    tof = []
    for t in t_signal:
        idx = bisect_left(t_pulse, t)
        if idx == len(t_pulse):
            t_0 = t_pulse.iloc[-1]
        else:
            t_0 = t_pulse[idx-1]
        tof.append((t - t_0)/1e6)  # convert to ps to us
    return tof


def calc_trans(counts_in, counts_out, t_meas_in, t_meas_out):
    """ calculate transmission and propagate error """
    vals_trans = [(x/t_meas_in)/(y/t_meas_out) if
                  y != 0 else 0 for x, y in zip(counts_in, counts_out)]
    err_trans = [(x/t_meas_in)/(y/t_meas_out)
                 * np.sqrt((1/np.sqrt(x))**2 + (1/np.sqrt(y))**2)
                 if y != 0 else 0 for x, y in zip(counts_in, counts_out)]
    return np.array(vals_trans), np.array(err_trans)


def calc_atten(data, thick, err_thick={}, keys=[], key_ref='target_out',
               bin_lo=90, bin_hi=135):
    """ calc transuation in number of counts """
    if keys == []:
        keys = list(data.keys())
        keys.remove(key_ref)
    if err_thick == {}:
        err_thick = {key: 0. for key in keys}
    trans = {}
    err_trans = {}
    mu = {}
    err_mu = {}
    c_in = sum(data[key_ref][bin_lo:bin_hi])
    for key in keys:
        c_out = sum(data[key][bin_lo:bin_hi])
        trans[key] = c_out / c_in
        err_trans[key] = trans[key] * np.sqrt(1/c_out + 1/c_in)
        mu[key] = -1*np.log(trans[key])/thick[key]
        err_mu[key] = np.sqrt((err_trans[key]/trans[key])**2
                              + (err_thick[key]/thick[key])**2)
        print(f'{key}:'.ljust(20) + f'mu = {mu[key]*10:.2f} '
              f'+/- {err_mu[key]*10:.2f}' + ' [cm-1]')
    return (trans, mu), (err_trans, err_mu)


def plot_2d(runs, key, var1, var2, filtered='unfiltered'):
    """Make a 2D plot for two variables."""
    run = runs[key]
    data = run.data[filtered]['CH0']
    data['PSD'] = 1 - (data['ENERGYSHORT'] / data['ENERGY'])
    plt.figure(figsize=(16, 9))
    x = data[var1]
    y = data[var2]
    plt.scatter(x, y, s=0.1, c='b', label=key)
    plt.xlim(left=0)
    plt.ylim([0., 1.])
    plt.xlabel(var1, labelpad=10)
    plt.ylabel(var2, labelpad=10)
    plt.legend()


def plot_e_psd(runs, key, filter_params={}, w=1.0):
    """Plot energy vs PSD."""
    data = runs[key].data['unfiltered']['CH0']
    data['PSD'] = 1 - (data['ENERGYSHORT'] / data['ENERGY'])
    data = df_filter(data, filter_params)
    heatmap, xedges, yedges = np.histogram2d(
        data['ENERGY']*w, data['PSD'],
        bins=400, range=[[0, 4000], [0, 1]]
    )
    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]
    cmap = mpl.cm.get_cmap("plasma").copy()
    cmap.set_under(color='white')
    plt.figure(figsize=(12, 12))
    plt.imshow(heatmap.T, extent=extent, origin='lower',
               vmin=0.5, vmax=25, cmap=cmap, aspect=2000)
    plt.xlabel('Energy [ADC]')
    plt.ylabel('PSD')
    plt.tight_layout()


def df_filter(df, filter_params):
    """Filter parameter by values."""
    df = df.copy()
    for key, value in filter_params.items():
        if value != ('0.0', '0.0'):
            df = df.loc[(df[key] >= float(value[0])) and
                        (df[key] <= float(value[1]))]
    return df


def plot_trans(runs, key_target, key_open,
               t_lo=0., t_hi=200., n_bins=400, t_offset=10.0,
               color='black', add_plot=False):
    """Calculate transmission and plot."""
    target_in = runs[key_target].data['filtered']['CH0']['TOF']
    target_out = runs[key_open].data['filtered']['CH0']['TOF']
    t_meas_in = runs[key_target].params['t_meas']
    t_meas_out = runs[key_open].params['t_meas']
    counts_in, __ = np.histogram(target_in, bins=n_bins, range=[t_lo, t_hi])
    counts_out, __ = np.histogram(target_out, bins=n_bins, range=[t_lo, t_hi])
    bins = np.linspace(t_lo, t_hi, n_bins+1)[:-1] + (
        (t_hi-t_lo)/n_bins/2 - t_offset)
    vals_trans, vals_errs = calc_trans(counts_in, counts_out,
                                       t_meas_in, t_meas_out)
    if not add_plot:
        plt.figure(figsize=(16, 9))
    plt.errorbar(x=bins, y=vals_trans, yerr=vals_errs,
                 lw=2, elinewidth=0.5, capsize=1, color=color,
                 label=key_target + ' transmission')
    plt.xlim([max(0, t_lo), t_hi-t_offset])
    plt.xlabel(r'TIME [$\mu$s]', labelpad=10)
    plt.ylabel(r'TRANSMISSION', labelpad=10)
    plt.legend()
    plt.tight_layout()
    return vals_trans, vals_errs, bins

if __name__ == '__main__':
    folders = ['C:/Users/Avram/Dropbox (MIT)/MIT/research/NRTA/Experiments/du-studies/DAQ/',
               # 'C:/Users/Avram/Dropbox (MIT)/Resonances/data/CoMPASS/20210531/DAQ/',
               'C:/Users/Avram/Dropbox (MIT)/MIT/research/NRTA/Experiments/IAP-2022/DAQ/']
    folder, key_tuples, VERBOSE = initialize(folders=folders)
    if VERBOSE:
        def verboseprint(*args, **kwargs):
            print(*args, **kwargs)
    else:
        def verboseprint(*args, **kwargs):
            return
    runs = process_runs(key_tuples, verbose=VERBOSE)
    keys = runs.keys()
